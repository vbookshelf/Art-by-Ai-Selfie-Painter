{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"**Objective**\n\nIn this notebook we will build a model that can transform a human portrait photo into an art image. This model will be deployed online as a prototype Tensorflow.js web app.\n\n**Background**\n\nThe current method to create a neural style transfer art image involves - choosing a style image, choosing a content image and then running back propogation on this content image for a specified number of iterations. Because this process takes a lot of time it isn't suitable for web deployment. Online users want fast results. \n\nTo ensure fast conversion of photos to art I started by using a neural style transfer alogrithm to create a dataset of 10,200 art images. Here we will train a U-Net cnn using portrait photos as the input data and those art images as the target data.\n\nThe art images are part of the Art by Ai dataset. The portrait photos are from the AISegment dataset.\n\nThe process used to create the Art by Ai dataset is explained in this notebook:<br>\nhttps://www.kaggle.com/vbookshelf/art-by-ai-how-to-create-the-dataset\n\n**Approach**\n\n- Use U-Net because it was created to ouput images and it's been designed to run fast.\n- Use data augmentation (horizontal flipping) to double the size of the available training data.\n- Use data generators to ensure that the 13GB kernel RAM limit is not exceeded.\n\n**Results**\n\nThe model is able to transform photos to art and the app runs fast. However, the model produces art images that don't match the multi-coloured style of the existing art training images. Duplicating this colouful effect might require more training data and more training epochs.\n\n> Web App:<br>\n> http://art.test.woza.work/\n> \n> Github:<br>\n> https://github.com/vbookshelf/Art-by-Ai-Selfie-Painter\n\nThe javascript, html and css code for the app is available on github. For best results, please access the app using the Chrome browser.\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# set seeds to ensure repeatability of results\nfrom numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport cv2\n#from scipy.misc import imsave\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\n\nfrom sklearn.model_selection import train_test_split\n\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for non image files in the folder. These can cause errors later.\n\n# get a list of art images that are in the folder\nart_images_list = os.listdir('../input/art-by-ai-neural-style-transfer/content_images/content_images')\n\nfor fname in art_images_list:\n    extension = fname.split('.')[1]\n    \n    # if not a jpg image then print the file name\n    if extension != 'jpg':\n        print(fname)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"IMG_HEIGHT = 400\nIMG_WIDTH = 400\nIMG_CHANNELS = 3\n\n# Set the number of images to use from the Art by Ai dataset.\nSAMPLE_SIZE = 10180\n\n# This batch size will be used for the train and val generators.\n# The train gen will output 2*BATCH_SIZE because it augments the images.\n# The test gen batch size is set at 1.\nBATCH_SIZE = 5\n\nLEARNING_RATE = 0.001\n\n# Tensorflow.js does not support unint8. Therefore we will use int32.\nIMG_DTYPE = 'int32'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display sample images"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(10,10))\nplt.axis('Off')\n\n# plt.subplot(nrows, ncols, plot_number)\n\nimage_id = '1803261926-00000039.jpg'\nfolder_id = 1803261926\n\n# == row 1 ==\n\n# image\nplt.subplot(1,2,1)\npath = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\nstr(folder_id) + '/clip_00000000/' + image_id\n\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Source Image', fontsize=12)\nplt.axis('off')\n\n# image\nplt.subplot(1,2,2)\npath = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Style Transfer Algo Image', fontsize=12)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(10,10))\nplt.axis('Off')\n\n# plt.subplot(nrows, ncols, plot_number)\n\nimage_id = '1803281444-00000390.jpg'\nfolder_id = 1803281444\n\n\n# == row 1 ==\n\n# image\nplt.subplot(1,2,1)\npath = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\nstr(folder_id) + '/clip_00000000/' + image_id\n\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Source Image', fontsize=12)\nplt.axis('off')\n\n# image\nplt.subplot(1,2,2)\npath = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Style Transfer Algo Image', fontsize=12)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Create Dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a list of 10 test set portraits\n\ntest_id_list = ['1803232244-00000007.jpg',\n '1803261926-00000039.jpg',\n '1803281444-00000390.jpg',\n '1803261926-00000434.jpg',\n '1803250811-00000315.jpg',\n '1803261926-00000106.jpg',\n '1803281444-00000166.jpg',\n '1803250811-00000358.jpg',\n '1803250811-00000339.jpg',\n '1803250936-00000547.jpg']\n\n\n# Load the list of art images in the Art by Ai dataset\nart_images_list = os.listdir('../input/art-by-ai-neural-style-transfer/content_images/content_images')\n\n# create a data frame with the file names of all art images\ndf_art = pd.DataFrame(art_images_list, columns=['image_id'])\n\n# create a test dataframe\ndf_test = pd.DataFrame(test_id_list, columns=['image_id'])\n\n# Reset the index.\ndf_test = df_test.reset_index(drop=True)\n\n\n# Select only rows that are not part of the test set.\n# Note the use of ~ to execute 'not in'.\ndf_data = df_art[~df_art['image_id'].isin(test_id_list)]\n\n\nprint(df_data.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test SplitÂ¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_test_split\n\n# Reduce the number of rows of df_data to speed up training.\n# Choose a random sample of rows.\ndf_data = df_data.sample(SAMPLE_SIZE, random_state=101)\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101)\n\n\n# reset the index\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\nprint(df_train.shape)\nprint(df_val.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save the dataframes as compressed csv files"},{"metadata":{},"cell_type":"markdown","source":"Having csv files will allow us to use Pandas chunking to feed images into the generators. Although not essential here, compression is very helpful when working with huge datasets because there's only 4.9GB of disk space available in the Kaggle kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the dataframes as a compressed csv files\n\ndf_train.to_csv('df_train.csv.gz', compression='gzip', index=False)\ndf_val.to_csv('df_val.csv.gz', compression='gzip', index=False)\ndf_test.to_csv('df_test.csv.gz', compression='gzip', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the files were saved\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Data Generators"},{"metadata":{},"cell_type":"markdown","source":"The ouput from a generator does not accumulate in memory. Each output batch overwrites the last one. This means that we can feed large amounts of data into a model without running out of RAM and crashing the kernel. There's a 13GB RAM limit when using a GPU.\n\nWe will use Pandas chunking and the compressed csv files to feed data into the generators. Using chunking simplifies the code. For example, the last batch that is fed into a generator will be smaller than the others. Pandas chunking will handle this change in batch size automatically which means that we won't need to write code to handle this condition.\n\nChunking is very useful when the csv file data is too large to be loaded into memory i.e. into a single Pandas dataframe."},{"metadata":{},"cell_type":"markdown","source":"**Notes**\n\n> - X_train images are sourced from the AISegment dataset of human portrait photos. The photos are 800x600 therefore, we will need to resize to 400x400 to suit the square shape that U-Net expects.\n> - Y_train is the art images associated with the above human portrait photos. These are part of the Art by Ai dataset. The size is 400x300. We will resize to 400x400.\n> - Both the portrait photos and their corresponding art images have the same file name.\n- The train gen will output a stacked matrix made up of orginal images and those same images that have been flipped horizontally. It outputs a X_train and Y_train matrices that are twice the size of the input batch size. Flipping the images doubles the amount of training data.\n- We won't normalize the data."},{"metadata":{},"cell_type":"markdown","source":"### [ 1 ] Train Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(batch_size=5):\n    \n    while True:\n        \n        # load the data in chunks (batches) from  ** df_train.csv.gz **\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of portrait images\n            image_id_list = list(df['image_id'])\n            # get list of art images\n            art_id_list = list(df['image_id'])\n            \n            \n            # X_train\n            # =========\n            \n            # create an empty matrix\n            X_orig = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n            X_hflip = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n\n\n            for i, image_id in enumerate(image_id_list):\n\n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\n                str(folder_id) + '/clip_00000000/' + image_id\n\n                # read the file as an array\n                image = plt.imread(path)\n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # original image\n                X_orig[i] = image\n                # flip image horizontally\n                X_hflip[i] = np.fliplr(image)\n\n                # stack the matrices to form X_train\n                X_train = np.vstack((X_orig, X_hflip))\n\n            \n            \n            # Y_train\n            # =========\n            \n            \n            # create an empty matrix\n            Y_orig = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n            Y_hflip = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n\n\n            for i, image_id in enumerate(art_id_list):\n\n                # set the path to the image\n                path = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\n\n                # read the file as an array\n                #image = imread(path)\n                # read the image using skimage\n                image = imread(path)\n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # original image\n                Y_orig[i] = image\n                # flip image horizontally\n                Y_hflip[i] = np.fliplr(image)\n\n                # stack the matrices to form Y_train\n                Y_train = np.vstack((Y_orig, Y_hflip))\n        \n        \n            yield X_train, Y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the train generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the generator\n\n# initialize\ntrain_gen = train_generator(batch_size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_train, Y_train = next(train_gen)\n\nprint(X_train.shape)\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print an image image from X_train\n\n# can also write\n#img = X_train[0]\n\nimg = X_train[0,:,:,:]\nplt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print an image from Y_train\n\n# can also write\n#img = Y_train[0]\n\nimg = Y_train[0,:,:,:]\nplt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [ 2 ] Val Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_generator(batch_size=5):\n    \n    while True:\n        \n        # load the data in chunks (batches) from  ** df_val.csv.gz **\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of portrait images\n            image_id_list = list(df['image_id'])\n            # get list of art images\n            art_id_list = list(df['image_id'])\n            \n            \n            # X_val\n            # =========\n            \n            # create an empty matrix\n            X_val = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n\n\n            for i, image_id in enumerate(image_id_list):\n\n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\n                str(folder_id) + '/clip_00000000/' + image_id\n\n                # read the file as an array\n                image = plt.imread(path)\n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n\n\n                # insert the image into X_val\n                X_val[i] = image\n                \n            \n            \n            # Y_val\n            # =========\n            \n            \n            # create an empty matrix\n            Y_val = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n\n\n            for i, image_id in enumerate(art_id_list):\n\n                # set the path to the image\n                path = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\n\n                # read the file as an array\n                #image = imread(path)\n                # read the image using skimage\n                image = imread(path)\n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n\n                # insert the image into X_val\n                Y_val[i] = image\n        \n        \n            yield X_val, Y_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the val generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the generator\n\n# initialize\nval_gen = val_generator(batch_size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_val, Y_val = next(val_gen)\n\nprint(X_val.shape)\nprint(Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print an image from X_val\n\n# can also write\n#img = X_val[0]\n\nimg = X_val[0,:,:,:]\nplt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print an image from Y_val\n\n# can also write\n#img = Y_val[0]\n\nimg = Y_val[0,:,:,:]\nplt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [ 3 ] Test Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_generator(batch_size=1):\n    \n    while True:\n        \n        # load the data in chunks (batches) from  ** df_test.csv.gz **\n        for df in pd.read_csv('df_test.csv.gz', chunksize=batch_size):\n            \n            # get the list of portrait images\n            image_id_list = list(df['image_id'])\n            \n            \n            # X_test\n            # =========\n            \n            # create an empty matrix\n            X_test = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE)\n\n\n            for i, image_id in enumerate(image_id_list):\n\n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\n                str(folder_id) + '/clip_00000000/' + image_id\n\n                # read the file as an array\n                image = imread(path)\n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n\n\n                # insert the image into X_test\n                X_test[i] = image\n                \n            \n            yield X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the test generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the generator\n\n# initialize\ntest_gen = test_generator(batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_test = next(test_gen)\n\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first image in X_test\n\n# can also write\n#img = X_test[0]\n\nimg = X_test[0,:,:,:]\nplt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## U-Net Model Architecture"},{"metadata":{},"cell_type":"markdown","source":"> U-Net: Convolutional Networks for Biomedical Image Segmentation<br>\n> Olaf Ronneberger, Philipp Fischer, Thomas Brox<br>\n> https://arxiv.org/abs/1505.04597"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input, UpSampling2D\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.optimizers import Adam\n\nimport tensorflow as tf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# source: https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n# Modified to ouput an image with 3 channels.\n\n\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\n\noutputs = Conv2D(3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = BATCH_SIZE\nval_batch_size = BATCH_SIZE\n\n# Test batch size will be 1.\n\n# determine num train steps\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\n# determine num val steps\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the generators\ntrain_gen = train_generator(batch_size=BATCH_SIZE)\nval_gen = val_generator(batch_size=BATCH_SIZE)\n\n\nmodel.compile(Adam(lr=LEARNING_RATE), loss='mean_squared_error')\n\n\nfilepath = \"model.h5\"\n\nearlystopper = EarlyStopping(patience=3, verbose=1)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min')\n\ncallbacks_list = [earlystopper, checkpoint]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=10, \n                              validation_data=val_gen, validation_steps=val_steps,\n                             verbose=1,\n                             callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the Training Curves"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmean_squared_error = history.history['loss']\nval_mean_squared_error = history.history['val_loss']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(mean_squared_error) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, mean_squared_error, 'bo', label='Training mse')\nplt.plot(epochs, val_mean_squared_error, 'b', label='Validation mse')\nplt.title('Training and validation mse')\nplt.legend()\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# initialize the test generator\ntest_gen = test_generator(batch_size=1)\n\n# use the best epoch\nmodel.load_weights(filepath = 'model.h5')\n\npreds = model.predict_generator(test_gen, \n                                steps=len(df_test), \n                                verbose=1)\n\n# check the max and min predicted pixel values\nprint('\\n')\nprint(preds.max())\nprint(preds.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clip the predicted pixel values to between 0 and 255.\n\npreds = np.clip(preds, 0, 255).astype(IMG_DTYPE)\n\n\n# check the max and min predicted pixel values again\nprint(preds.max())\nprint(preds.min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Test Set Results"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.axis('Off')\n\n# Our subplot will contain 3 rows and 3 columns\n# plt.subplot(nrows, ncols, plot_number)\n\nimage_id = '1803261926-00000039.jpg'\nfolder_id = 1803261926\n\n# == row 1 ==\n\n# image\nplt.subplot(1,3,1)\npath = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\nstr(folder_id) + '/clip_00000000/' + image_id\n\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Source Image', fontsize=20)\nplt.axis('off')\n\n# image\nplt.subplot(1,3,2)\npath = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Style Transfer Algo Image', fontsize=20)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,3,3)\nimage = preds[1]\nplt.imshow(image)\nplt.title('CNN Predicted Image', fontsize=20)\nplt.axis('off')\n\nplt.show()#","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.axis('Off')\n\n# Our subplot will contain 3 rows and 3 columns\n# plt.subplot(nrows, ncols, plot_number)\n\nimage_id = '1803281444-00000390.jpg'\nfolder_id = 1803281444\n\n# == row 1 ==\n\n# image\nplt.subplot(1,3,1)\npath = '../input/aisegmentcom-matting-human-datasets/matting_human_half/clip_img/' + \\\nstr(folder_id) + '/clip_00000000/' + image_id\n\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Source Image', fontsize=20)\nplt.axis('off')\n\n# image\nplt.subplot(1,3,2)\npath = '../input/art-by-ai-neural-style-transfer/content_images/content_images/' + image_id\nimage = plt.imread(path)\nplt.imshow(image)\nplt.title('Style Transfer Algo Image', fontsize=20)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,3,3)\nimage = preds[2]\nplt.imshow(image)\nplt.title('CNN Predicted Image', fontsize=20)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The app will re-size the predicted image to 320x240 before displaying it on the web page. "},{"metadata":{},"cell_type":"markdown","source":"## Convert the Model to Tensorflow.js"},{"metadata":{},"cell_type":"markdown","source":"One challenge that I find with Tensorflow.js is that the model conversion process is not robust. Here I've had to use workarounds to address two errors that ocurred during model conversion. This technology is still maturing so these bugs are to be expected. Even with these glitches, Tensorflow.js is still a fantastic tool."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Stackoverflow solutions to errors:\n\n# https://stackoverflow.com/questions/49932759/pip-10-and-apt-how-to-avoid-cannot-uninstall\n# -x-errors-for-distutils-packages\n\n# https://stackoverflow.com/questions/56003095/no-add-to-collection-was-found-when-\n# using-tensorflowjs-converter\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Install tensorflowjs.\n# Don't use the latest version. Instead install version 1.1.2\n\n# --ignore-installed is added to fix an error.\n\n!pip install tensorflowjs==1.1.2 --ignore-installed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs/model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that the folder containing the tfjs model files has been created\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Here we used this workflow to generate art. I believe that this workflow can be adapted for use in other areas like medicine or forensics - in fact any use-case that requires a web based Ai tool that outputs an image.\n\nThank you for reading."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}